{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d1b1cd-3c00-4a34-9ace-8fc84ad3577b",
   "metadata": {},
   "source": [
    "This script calculates auPR, auROC and F1 scores for test portion of the Topic CREs, the non-augmented version to benchmark models against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db76c54-88d8-4bce-ac91-8cd6490253a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import crested\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965a90a-8a5c-4664-880f-084240b5c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='/home/DeepHB/'\n",
    "out_Dir='/home/DeepHB/Outs/'\n",
    "os.chdir(out_Dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcda5cd-bbd9-49b8-8747-e3e99eb5be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_files=dataDir+'TopicMP_CREs'\n",
    "chromsizes=dataDir+'extrafiles/chromSizes_canchrom.genome'\n",
    "adata = crested.import_beds(\n",
    "    beds_folder=bed_files, chromsizes_file=chromsizes\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa740bd-823f-4856-bc6c-c16f81a0e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the test split\n",
    "with open('Outs/deephb_testsplit_cnn_57MP.pkl', 'rb') as f:\n",
    "    test_part=pickle.load( f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed7b0d8-887f-45d3-aff5-089eb81faa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = test_part.index.astype(str).to_list()\n",
    "arr2 = df.index.astype(str).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892a288-6022-4b08-974b-d9263978f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = set(arr1) & set(arr2)\n",
    "print(len(arr1))\n",
    "print(len(arr2))\n",
    "print(len(common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc81ff-2898-4693-b1a7-cd5ae13730c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common is test part of adata, it is 10%\n",
    "adata_subset = adata[:, list(common)]\n",
    "adata_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db56429-7642-41e9-937e-f3e6775f2851",
   "metadata": {},
   "source": [
    "#### Now loading the models and running prediction to obtain metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30376978-8748-4b17-974f-c47ff8e8a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "genome_fasta=dataDir+'extrafiles/gencdH38p13r37.fa'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3519e-4977-47e0-b449-e8a637da2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load model\n",
    "model_path = \"deephb_cnn_57MP_99.keras\"\n",
    "#model_path = deephb_lstmv1_57MP_53.keras\"\n",
    "#model_path = \"deephb_lstmv2_57MP_73.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66fe96-922f-453f-8e2c-6aba91b4ea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model(\n",
    "    model_path, compile=False\n",
    ")  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9aa09-4ceb-4da8-862b-b16401855297",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = crested.tl.predict(adata_subset, model,genome=genome_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df571b39-3b51-4e5f-82c6-886bacbcea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('HBCRE_test/predHBCRE_test_deephb_cnn_57MP_99.pkl'), 'wb') as f:\n",
    "      pickle.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b913cc-6537-440f-92ef-4304be3b18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join('HBCRE_test/predHBCRE_test_deephb_lstmv1_57MP_53.pkl'), 'wb') as f:\n",
    "#     pickle.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35f4d0-81da-420b-a22e-170eecef2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join('HBCRE_test/predHBCRE_test_deephb_lstmv2_57MP_73.pkl'), 'wb') as f:\n",
    "#      pickle.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682bdacf-d8c0-4380-89c0-602c35487049",
   "metadata": {},
   "source": [
    "#### Obtianing metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab0e9c-f46f-42c9-bc7a-19f0d7a60ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataDir+'TopicMPs.txt'), \"r\") as file:\n",
    "    labels = file.read().splitlines()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f13ee4-fec1-4fd6-a968-02791f67469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt=adata_subset.X.T #ground turth\n",
    "print(gt.shape)\n",
    "gt_rs=gt.sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979098a-7ef2-4de7-9e8d-b7ac4511cd7e",
   "metadata": {},
   "source": [
    "## F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde083a-dc84-4844-9246-fc6b28e905f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7356a9-a3f4-45be-8b99-19853b8725ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actual binary labels and predicted scores\n",
    "# gt: ground truth, shape (n_CREs, 57)\n",
    "# prediction: obtained score, shape (n_CREs, 57)\n",
    "\n",
    "best_thresholds = []\n",
    "best_f1s = []\n",
    "all_metrics = []\n",
    "\n",
    "thresholds = np.arange(0.00, 1.01, 0.01)\n",
    "\n",
    "for i in range(57):  # For each Topic MP \n",
    "    y_t = gt[:, i]\n",
    "    y_s = prediction[:, i]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0\n",
    "    metrics_for_factor = []\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_s >= thresh).astype(int)\n",
    "        precision = precision_score(y_t, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_t, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_t, y_pred, zero_division=0)\n",
    "\n",
    "        metrics_for_factor.append((thresh, precision, recall, f1))\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "\n",
    "    best_thresholds.append(best_thresh)\n",
    "    best_f1s.append(best_f1)\n",
    "    all_metrics.append(metrics_for_factor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3bd84-ec30-404e-803b-dc98f29e264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save best thresholdsimport pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "## 67 classes\n",
    "for idx in range(57):\n",
    "    # Extract metrics at the best threshold\n",
    "    best_thresh = best_thresholds[idx]\n",
    "    metrics = next((m for m in all_metrics[idx] if m[0] == best_thresh), None)\n",
    "\n",
    "    summary_data.append({\n",
    "        \"MP\": labels[idx],\n",
    "        \"Best Threshold\": best_thresh,\n",
    "        \"Precision\": metrics[1],\n",
    "        \"Recall\": metrics[2],\n",
    "        \"F1 Score\": metrics[3]\n",
    "    })\n",
    "\n",
    "#print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60ae5d-9789-4bb9-bf4f-a330daaf0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(\"HBCRE_test/MP_threshold_summary_deephb_cnn_57MP_99.txt\", sep='\\t')\n",
    "#summary_df.to_csv(\"HBCRE_test/MP_threshold_summary_deephb_lstmv1_57MP_53.txt\", sep='\\t')\n",
    "#summary_df.to_csv(\"HBCRE_test/MP_threshold_summary_deephb_lstmv2_57MP_73.txt\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620e879-5e92-44eb-a0f9-081fbdaf5475",
   "metadata": {},
   "source": [
    "## auPR and auROC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3b017-35f9-46f9-8cf0-19c81e28009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_subset.layers[\"predict\"] = prediction.T  # adata expects (C, N) instead of (N, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210c679-0437-491a-9ee1-f1e0081477b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96210d4-55df-47f2-81b9-514c66f39c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "auPR_values = []\n",
    "auROC_values = []\n",
    "\n",
    "\n",
    "for i in range(gt.shape[1]):\n",
    "    roc = tf.keras.metrics.AUC(curve='ROC')\n",
    "    roc.update_state(gt[:, i], prediction[:, i])\n",
    "    auROC = roc.result().numpy()\n",
    "    auROC_values.append(auROC)\n",
    "    \n",
    "    pr = tf.keras.metrics.AUC(curve='PR')\n",
    "    pr.update_state(gt[:, i], prediction[:, i])\n",
    "    auPR = pr.result().numpy()\n",
    "    auPR_values.append(auPR)\n",
    "    \n",
    "\n",
    "    #print(f\"Class {labels[i]}: auROC={auROC:.4f}, auPR={auPR:.4f}\")\n",
    "\n",
    "macro_auPR = np.mean(auPR_values)\n",
    "macro_auROC = np.mean(auROC_values)\n",
    "\n",
    "print(f\"Macro-Average auPR: {macro_auPR}\")\n",
    "print(f\"Macro-Average auROC: {macro_auROC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dc3f2-c418-47c3-826c-578e69bf1758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain values for box plot\n",
    "data = {'ROC': auROC_values, 'PR': auPR_values}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame (optional)\n",
    "#print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('HBCRE_test/ROC_PR_deephb_cnn_99.txt', index=False,sep='\\t')\n",
    "#df.to_csv('HBCRE_test/ROC_PR_deephb_lstmv1_53.txt', index=False,sep='\\t')\n",
    "#df.to_csv('HBCRE_test/ROC_PR_deephb_lstmv2_73.txt', index=False,sep='\\t')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested2",
   "language": "python",
   "name": "crested2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
